{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "FrozenLak.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "rNbEjVanTAV7"
      },
      "source": [
        "import gym\n",
        "import random\n",
        "import numpy as np\n",
        "import time\n",
        "from gym.envs.registration import register\n",
        "from IPython.display import clear_output"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RfOoHaMPTH4M",
        "outputId": "57a832cc-004a-4395-ab49-e626ce128a4a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "try:\n",
        "    register(\n",
        "    id='FrozenLakeNoSlip-v0',\n",
        "    entry_point='gym.envs.toy_text:FrozenLakeEnv',\n",
        "    kwargs={'map_name' : '4x4','is_slippery':False},\n",
        "    max_episode_steps=100,\n",
        "    reward_threshold=0.78, # optimum = .8196\n",
        ")\n",
        "except:\n",
        "  pass\n",
        "\n",
        "env_name = \"FrozenLakeNoSlip-v0\"\n",
        "env = gym.make(env_name)\n",
        "print(\"observation space:\",env.observation_space)\n",
        "print(\"Action space\", env.action_space)\n",
        "type(env.action_space)"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "observation space: Discrete(16)\n",
            "Action space Discrete(4)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "gym.spaces.discrete.Discrete"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NsLILKwpTz0t"
      },
      "source": [
        "class Agent():\n",
        "  def __init__(self,env):\n",
        "    self.is_discrete = \\\n",
        "        type(env.action_space) == gym.spaces.discrete.Discrete\n",
        "\n",
        "    if self.is_discrete:\n",
        "      self.action_size = env.action_space.n\n",
        "      print(\"Action size:\", self.action_size)\n",
        "    else:\n",
        "      self.action_low = env.action_space.low\n",
        "      self.action_high = env.action_space.high\n",
        "      self.action_shape = env.action_space.shape\n",
        "      pring(\"Action range:\", self.action_low, self.action_high)\n",
        "\n",
        "  def get_action(self, state):\n",
        "    if self.is_discrete:\n",
        "      action = random.choice(range(self.action_size))\n",
        "    else:\n",
        "      action = np.random.uniform(self.action_low,\n",
        "                                 self.action_high,\n",
        "                                 self.action_shape)\n",
        "    return action     "
      ],
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lNrbD-hVdJwY",
        "outputId": "5e58598e-f175-4895-a28d-0ef3bbdac4c6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "class QAgent(Agent):\n",
        "  def __init__(self, env, discount_rate=0.97, learning_rate=0.01):\n",
        "    super().__init__(env)\n",
        "    self.state_size = env.observation_space.n\n",
        "    print(\"State size:\", self.state_size)\n",
        "\n",
        "    self.eps = 1.0\n",
        "    self.discount_rate = discount_rate\n",
        "    self.learning_rate = learning_rate\n",
        "    self.build_model()\n",
        "\n",
        "  def build_model(self):\n",
        "    self.q_table = 1e-4*np.random.random([self.state_size, self.action_size])\n",
        "  \n",
        "  def get_action(self,state):\n",
        "    q_state = self.q_table[state]\n",
        "    action_greedy = np.argmax(q_state)\n",
        "    action_random = super().get_action(state)\n",
        "    return action_random if random.random() < self.eps else action_greedy\n",
        "  \n",
        "  def train(self, experience):\n",
        "    state, action, next_state, reward, done = experience\n",
        "\n",
        "    q_next = self.q_table[next_state]\n",
        "    q_next = np.zeros([self.action_size]) if done else q_next\n",
        "    q_target = reward + self.discount_rate * np.max(q_next)\n",
        "\n",
        "    q_update = q_target - self.q_table[state,action]\n",
        "    self.q_table[state,action] += self.learning_rate * q_update\n",
        "\n",
        "    if done:\n",
        "      self.eps = self.eps * 0.99\n",
        "\n",
        "agent = QAgent(env)"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Action size: 4\n",
            "State size: 16\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YHz9rAyFXVzi",
        "outputId": "7a0e3b7b-302a-4dbe-f106-8be8bd9d0631",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "total_reward = 0\n",
        "for ep in range(100):\n",
        "  state = env.reset()\n",
        "  done = False\n",
        "  while not done:\n",
        "    # action = env.action_space.sample()\n",
        "    action = agent.get_action(state)\n",
        "    next_state, reward, done, info = env.step(action)\n",
        "    agent.train((state,action,next_state,reward,done))\n",
        "    state = next_state\n",
        "    total_reward += reward\n",
        "\n",
        "    print(\"s:\", state, \"a:\", action)\n",
        "    print(\"Episode: {}, Total reward: {}, eps: {}\".format(ep,total_reward,agent.eps))\n",
        "    env.render()\n",
        "    print(agent.q_table)\n",
        "    time.sleep(0.01)\n",
        "    clear_output(wait=True)"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "s: 15 a: 2\n",
            "Episode: 99, Total reward: 23.0, eps: 0.13397967485796175\n",
            "  (Right)\n",
            "SFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFF\u001b[41mG\u001b[0m\n",
            "[[5.34807558e-05 6.23788597e-05 7.59241701e-05 6.83093877e-05]\n",
            " [4.44309764e-05 1.63302909e-06 8.25222045e-05 8.24286461e-05]\n",
            " [8.26220735e-05 9.83830263e-05 5.96644452e-06 2.75342553e-05]\n",
            " [1.44793362e-05 7.30713313e-06 1.51810832e-05 1.60634876e-05]\n",
            " [3.32765924e-05 7.43314561e-05 5.62036268e-05 3.45581327e-05]\n",
            " [7.17559051e-05 1.89918878e-05 6.68216476e-05 7.37204945e-05]\n",
            " [5.61371072e-05 9.76108108e-04 7.18073871e-05 1.61409355e-05]\n",
            " [9.89744784e-05 6.82512579e-05 7.84283607e-05 6.80111574e-06]\n",
            " [5.86680179e-05 5.55721286e-05 1.16126970e-04 2.37377455e-05]\n",
            " [7.87474042e-05 1.02721375e-04 1.04677870e-03 3.35784308e-05]\n",
            " [2.83187874e-05 2.14621364e-02 4.74720700e-05 6.28310081e-05]\n",
            " [8.38986139e-05 3.57242815e-05 8.17481368e-05 4.72744605e-05]\n",
            " [7.48207740e-06 4.37286260e-05 5.32596194e-05 6.09696504e-05]\n",
            " [4.39785602e-05 8.90330057e-05 3.60635648e-03 3.23661746e-05]\n",
            " [6.79068937e-05 2.02078201e-03 2.14342744e-01 5.95499313e-05]\n",
            " [2.40887348e-05 9.75650333e-05 7.59122287e-05 1.87485652e-05]]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}