{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "anaconda-cloud": {},
    "colab": {
      "name": "2.1-FNN-NumPy.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HAva8TnYFtFu"
      },
      "source": [
        "# Contents and why we need this lab\n",
        "\n",
        "This lab is about implementing neural networks yourself in NumPy before we start using other frameworks which hide some of the computation from you. It builds on the first lab where you derived the equations for neural network forward and backward propagation and gradient descent parameter updates."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sCa7HzwpFtFy"
      },
      "source": [
        "# External sources of information\n",
        "\n",
        "1. Jupyter notebook. You can find more information about Jupyter notebooks [here](https://jupyter.org/). It will come as part of the [Anaconda](https://www.anaconda.com/) Python installation. \n",
        "2. [NumPy](https://numpy.org/). Part of Anaconda distribution. If you already know how to program most things about Python and NumPy can be found through Google search. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1SjiIp-TFtF0"
      },
      "source": [
        "# This notebook will follow the next steps:\n",
        "\n",
        "1. Data generation\n",
        "2. Initialization of parameters\n",
        "3. Definition of activation functions   \n",
        "4. A short explanation of numpy's einsum function\n",
        "5. Forward pass\n",
        "6. Backward pass (backward pass and finite differences)\n",
        "7. Training loop \n",
        "8. Testing your model\n",
        "9. Further extensions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VvtTIP_qFtF2"
      },
      "source": [
        "# Create an artificial dataset to play with\n",
        "\n",
        "We create a non-linear 1d regression task. The generator supports various noise levels and it creates train, validation and test sets. You can modify it yourself if you want more or less challenging tasks."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PB-yRe4aFtF5"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "np.random.seed(42)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "smI4QFhzFtGH"
      },
      "source": [
        "def data_generator(noise=0.1, n_samples=300, D1=True):\n",
        "    # Create covariates and response variable\n",
        "    if D1:\n",
        "        X = np.linspace(-3, 3, num=n_samples).reshape(-1,1) # 1-D\n",
        "        np.random.shuffle(X)\n",
        "        y = np.random.normal((0.5*np.sin(X[:,0]*3) + X[:,0]), noise) # 1-D with trend\n",
        "    else:\n",
        "        X = np.random.multivariate_normal(np.zeros(3), noise*np.eye(3), size = n_samples) # 3-D\n",
        "        np.random.shuffle(X)    \n",
        "        y = np.sin(X[:,0]) - 5*(X[:,1]**2) + 0.5*X[:,2] # 3-D\n",
        "\n",
        "    # Stack them together vertically to split data set\n",
        "    data_set = np.vstack((X.T,y)).T\n",
        "    \n",
        "    train, validation, test = np.split(data_set, [int(0.35*n_samples), int(0.7*n_samples)], axis=0)\n",
        "    \n",
        "    # Standardization of the data, remember we do the standardization with the training set mean and standard deviation\n",
        "    train_mu = np.mean(train, axis=0)\n",
        "    train_sigma = np.std(train, axis=0)\n",
        "    \n",
        "    train = (train-train_mu)/train_sigma\n",
        "    validation = (validation-train_mu)/train_sigma\n",
        "    test = (test-train_mu)/train_sigma\n",
        "    \n",
        "    x_train, x_validation, x_test = train[:,:-1], validation[:,:-1], test[:,:-1]\n",
        "    y_train, y_validation, y_test = train[:,-1], validation[:,-1], test[:,-1]\n",
        "\n",
        "    return x_train, y_train,  x_validation, y_validation, x_test, y_test"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FKvkAODOFtGQ"
      },
      "source": [
        "D1 = True\n",
        "x_train, y_train,  x_validation, y_validation, x_test, y_test = data_generator(noise=0.5, D1=D1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "im2-MOcwFtGW"
      },
      "source": [
        "if D1:\n",
        "    plt.scatter(x_train[:,0], y_train);\n",
        "    plt.scatter(x_validation[:,0], y_validation);\n",
        "    plt.scatter(x_test[:,0], y_test);\n",
        "else:\n",
        "    plt.scatter(x_train[:,1], y_train);\n",
        "    plt.scatter(x_validation[:,1], y_validation);\n",
        "    plt.scatter(x_test[:,1], y_test);\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QUMD2aNKFtGd"
      },
      "source": [
        "# Initialization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VbjrqcpVFtGe"
      },
      "source": [
        "The steps to create a feed forward neural network are the following:\n",
        "\n",
        "1. **Number of hidden layer and hidden units**. We have to define the number of hidden units in each layer. We are going to save these numbers in a list \"L\" that is going to start with our input dimensionality (the number of features in X) and is going to finish with our output dimensionality (the size of Y). Anything in between these values are going to be hidden layers and the number of hidden units in each hidden layer is defined by the researcher. Remember that for each unit in each layer (besides the first one, according to our list L) there is a bias term.\n",
        "2. **Activation functions** for each hidden layer. Each hidden layer in your list must have an activation function (it can also be the linear activation which is equivalent to identity function). The power of neural networks comes from non-linear activation functions that learn representations (features) from the data allowing us to learn from it. \n",
        "3. **Parameter initialization**. We will initialize the weights to have random values. This is done in practice by drawing pseudo random numbers from a Gaussian or uniform distribution. It turns out that for deeper models we have to be careful about how we scale the random numbers. This will be the topic of the exercise below. For now we will just use unit variance Gaussians.  \n",
        "\n",
        "Our initialization will work as follows: \n",
        "\n",
        "For each layer of the neural network defined in L, initialize a matrix of weights of size (units_in, units_out) from a random normal distribution [np.random.normal()](https://docs.scipy.org/doc/numpy-1.15.0/reference/generated/numpy.random.normal.html) and save them in a list called \"layers\". For each layer in our neural network, initialize a matrix of weights of size (1, units_out) as above and save them in a list called \"bias\". The function should return a tuple (layers, bias). The length of our lists must be len(L)-1."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MrcirTTHFtGf"
      },
      "source": [
        "# Initialize neural network:\n",
        "# the NN is a tuple with a list with weights and list with biases\n",
        "def init_NN(L):\n",
        "    \"\"\"\n",
        "    Function that initializes our feed-forward neural network. \n",
        "    Input: \n",
        "    L: list of integers. The first element must be equal to the number of features of x and the last element \n",
        "        must be the number of outputs in the network.\n",
        "    Output:\n",
        "    A tuple of:\n",
        "    weights: a list with randomly initialized weights of shape (in units, out units) each. The units are the ones we defined in L.\n",
        "        For example, if L = [2, 3, 4] layers must be a list with a first element of shape (2, 3) and a second elemtn of shape (3, 4). \n",
        "        The length of layers must be len(L)-1\n",
        "    biases: a list with randomly initialized biases of shape (1, out_units) each. For the example above, bias would be a list of length\n",
        "        2 with a first element of shape (1, 3) and a second element of shape (1, 4).\n",
        "    \"\"\"\n",
        "    weights = []\n",
        "    biases  = []\n",
        "    for i in range(len(L)-1):\n",
        "        weights.append(np.random.normal(loc=0.0, scale=1.0, size=[L[i],L[i+1]])) \n",
        "        biases.append(np.random.normal(loc=0.0, scale=1.0, size=[1, L[i+1]]))     \n",
        "        \n",
        "    return (weights, biases)\n",
        "\n",
        "# Initialize the unit test neural network:\n",
        "# Same steps as above but we will not initialize the weights randomly.\n",
        "def init_NN_UT(L):\n",
        "    weights = []\n",
        "    biases  = []\n",
        "    for i in range(len(L)-1):\n",
        "        weights.append(np.ones((L[i],L[i+1]))) \n",
        "        biases.append(np.ones((1, L[i+1])))     \n",
        "        \n",
        "    return (weights, biases)\n",
        "\n",
        "# Initializer the unit test neural network\n",
        "L_UT  = [3, 5, 1]\n",
        "NN_UT = init_NN_UT(L_UT)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oLrGJytZFtGm"
      },
      "source": [
        "## Exercise a) Print all network parameters\n",
        "\n",
        "Make a function that prints all parameters (weights and biases) with information about in which layer the parameters are."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iac-VwYGFtGm"
      },
      "source": [
        "# Insert code here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b-jdEl-7FtGs"
      },
      "source": [
        "# Advanced initialization schemes\n",
        "\n",
        "If we are not careful with initialization, the signals we propagate forward ($a^{(l)}$, $l=1,\\ldots,L$) and backward ($\\delta^l$, $l=L,L-1,\\ldots,1$) can blow up or shrink to zero. A statistical analysis of the variance of the signals for different activation functions can be found in these two papers: [Glorot initialization](http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf) and [He initialization](https://arxiv.org/pdf/1502.01852v1.pdf). \n",
        "\n",
        "The result of the analyses are proposals for how to make the initialization such that the variance of the signals (forward and backward) are kept approxmimatly constant when propagating from layer to layer. The exact expressions depend upon the non-linear activation function used. In Glorot initialization, the aim is to keep both the forward and backward variances constant whereas He only aims at keeping the variance in the forward pass constant.\n",
        "\n",
        "We define $n_{in}$ and $n_{out}$ as the number of input units and output units of a particular layer. \n",
        "\n",
        "The Glorot initialization has the form: \n",
        "\n",
        "$$w_{ij} \\sim N \\bigg( 0, \\, \\frac{2 \\alpha }{n_{in} + n_{out}} \\bigg) \\ . $$\n",
        "\n",
        "where $N(\\mu,\\sigma^2)$ is a Gaussian distribution with mean $\\mu$ and variance $\\sigma^2$ and $\\alpha$ is a parameter that depends upon the activation function used. For $\\tanh$, $\\alpha=1$ and for Rectified Linear Unit (ReLU) activations, $\\alpha=2$. (It is also possible to use a uniform distribution for initialization, see [this blog post](https://mmuratarat.github.io/2019-02-25/xavier-glorot-he-weight-init).) \n",
        "\n",
        "The He initialization is very similar\n",
        "\n",
        "$$w_{ij} \\sim N \\bigg( 0, \\, \\frac{\\alpha}{n_{in}} \\bigg) \\ . $$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mqeyab9qFtGs"
      },
      "source": [
        "## Exercise b) Glorot and He initialization\n",
        "\n",
        "Implement these initialization schemes by modifying the code given below.\n",
        "\n",
        "**NOTE:** The Gaussian is defined as $N( \\mu, \\, \\sigma^{2})$ but Numpy takes $\\sigma$ as argument.\n",
        "\n",
        "Explain briefly how you would test numerically that these initializations have the sought after property. Hint: See plots in Glorot paper."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qyk01CgaFtGt"
      },
      "source": [
        "## Glorot\n",
        "def init_NN_glorot_Tanh(L):\n",
        "    \"\"\"\n",
        "    Initializer using the glorot initialization scheme\n",
        "    \"\"\"\n",
        "    weights = []\n",
        "    biases  = []\n",
        "    for i in range(len(L)-1):\n",
        "        std = 1.0 # <- replace with proper initialization\n",
        "        weights.append(np.random.normal(loc=0.0, scale=std, size=[L[i],L[i+1]])) \n",
        "        biases.append(np.random.normal(loc=0.0, scale=std, size=[1, L[i+1]]))       \n",
        "        \n",
        "    return (weights, biases)\n",
        "\n",
        "## He\n",
        "def init_NN_he_ReLU(L):\n",
        "    \"\"\"\n",
        "    Initializer using the He initialization scheme\n",
        "    \"\"\"\n",
        "    weights = []\n",
        "    biases  = []\n",
        "    for i in range(len(L)-1):\n",
        "        std = 1.0 # <- replace with proper initialization\n",
        "        weights.append(np.random.normal(loc=0.0, scale=std, size=[L[i],L[i+1]])) \n",
        "        biases.append(np.random.normal(loc=0.0, scale=std, size=[1, L[i+1]]))       \n",
        "        \n",
        "    return (weights, biases)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-u4xk_ORFtGz"
      },
      "source": [
        "# Activation functions\n",
        "\n",
        "To have a full definition of the neural network, we must define an activation function for every layer in our list L (again, exluding the first term, which is the number of input dimensions). Several activation functions have been proposed and have different characteristics. Here, we will implement the linear activation function (the identity function), the sigmoid activation function (squeeshes the outcome of each neuron into the $[0, 1]$ range), the Hyperbolic Tangent (Tanh) that squeeshes the outcome of each neuron to $[-1, 1]$ and the Rectified Linear Unit (ReLU). \n",
        "\n",
        "We will also include the derivative in the function. We need this in order to do our back-propagation algorithm. Don't rush, we will get there soon. For any neural network, save the activation functions in a list. This list must be of size len(L)-1."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9MkoJmzaFtGz"
      },
      "source": [
        "## Linear activation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "3NVVlnwHFtG0"
      },
      "source": [
        "def Linear(x, derivative=False):\n",
        "    \"\"\"\n",
        "    Computes the element-wise Linear activation function for an array x\n",
        "    inputs:\n",
        "    x: The array where the function is applied\n",
        "    derivative: if set to True will return the derivative instead of the forward pass\n",
        "    \"\"\"\n",
        "    \n",
        "    if derivative:              # Return the derivative of the function evaluated at x\n",
        "        return np.ones_like(x)\n",
        "    else:                       # Return the forward pass of the function at x\n",
        "        return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Mm1k6xfFtG4"
      },
      "source": [
        "## Sigmoid activation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "dBmjKesUFtG6"
      },
      "source": [
        "def Sigmoid(x, derivative=False):\n",
        "    \"\"\"\n",
        "    Computes the element-wise Sigmoid activation function for an array x\n",
        "    inputs:\n",
        "    x: The array where the function is applied\n",
        "    derivative: if set to True will return the derivative instead of the forward pass\n",
        "    \"\"\"\n",
        "    f = 1/(1+np.exp(-x))\n",
        "    \n",
        "    if derivative:              # Return the derivative of the function evaluated at x\n",
        "        return f*(1-f)\n",
        "    else:                       # Return the forward pass of the function at x\n",
        "        return f"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S0ZULT1rFtG_"
      },
      "source": [
        "## Hyperbolic Tangent activation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "fESIXSJIFtHA"
      },
      "source": [
        "def Tanh(x, derivative=False):\n",
        "    \"\"\"\n",
        "    Computes the element-wise Sigmoid activation function for an array x\n",
        "    inputs:\n",
        "    x: The array where the function is applied\n",
        "    derivative: if set to True will return the derivative instead of the forward pass\n",
        "    \"\"\"\n",
        "    f = (np.exp(x)-np.exp(-x))/(np.exp(x)+np.exp(-x))\n",
        "    \n",
        "    if derivative:              # Return the derivative of the function evaluated at x\n",
        "        return 1-f**2\n",
        "    else:                       # Return the forward pass of the function at x\n",
        "        return f"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UVEQqh4LFtHF"
      },
      "source": [
        "## Rectifier linear unit (ReLU)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "wtG8BHKZFtHG"
      },
      "source": [
        "def ReLU(x, derivative=False):\n",
        "    \"\"\"\n",
        "    Computes the element-wise Rectifier Linear Unit activation function for an array x\n",
        "    inputs:\n",
        "    x: The array where the function is applied\n",
        "    derivative: if set to True will return the derivative instead of the forward pass\n",
        "    \"\"\"\n",
        "    \n",
        "    if derivative:              # Return the derivative of the function evaluated at x\n",
        "        return (x>0).astype(int)\n",
        "    else:                       # Return the forward pass of the function at x\n",
        "        return np.maximum(x, 0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_79HOAXrFtHK"
      },
      "source": [
        "## Visualization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RME0JjT_FtHK"
      },
      "source": [
        "Now that we have defined our activation functions we can visualize them to see what they look like:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oOL2UolJFtHL"
      },
      "source": [
        "x = np.linspace(-6, 6, 100)\n",
        "units = {\n",
        "    \"Linear\": lambda x: Linear(x),\n",
        "    \"Sigmoid\": lambda x: Sigmoid(x),\n",
        "    \"ReLU\": lambda x: ReLU(x),\n",
        "    \"tanh\": lambda x: Tanh(x)\n",
        "}\n",
        "\n",
        "plt.figure(figsize=(5, 5))\n",
        "[plt.plot(x, unit(x), label=unit_name, lw=2) for unit_name, unit in units.items()]\n",
        "plt.legend(loc=2, fontsize=16)\n",
        "plt.title('Our activation functions', fontsize=20)\n",
        "plt.ylim([-2, 5])\n",
        "plt.xlim([-6, 6])\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pnrJX1SGFtHR"
      },
      "source": [
        "## Exercise c) Glorot initialization for all activation functions\n",
        "\n",
        "Implement a function by adding to the code snippet below that can take network L and list of activations function as argument and return a Glorot initialized network.  Hint: [This blog post](https://mmuratarat.github.io/2019-02-25/xavier-glorot-he-weight-init) gives a table for the activation functions we use here.\n",
        "\n",
        "Briefly explain in words how these how these values are calculated."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "qW5UjBhzFtHR"
      },
      "source": [
        "def init_NN_Glorot(L, activations):\n",
        "    \"\"\"\n",
        "    Initializer using the glorot initialization scheme\n",
        "    \"\"\"\n",
        "    # Insert code here\n",
        "\n",
        "# Initializes the unit test neural network\n",
        "L_UT  = [3, 5, 1]\n",
        "ACT_UT = [ReLU, Tanh]\n",
        "NN_Glorot = init_NN_Glorot(L_UT, ACT_UT)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-JlvZ6O5FtHV"
      },
      "source": [
        "# Numpy einsum (EINstein SUMmation)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zq_yqPEIFtHX"
      },
      "source": [
        "[Einsum](https://docs.scipy.org/doc/numpy/reference/generated/numpy.einsum.html) gives us the possibility to compute almost any matrix operation in a single function. You can find a good description in the link above. Here are a few examples of some important uses:\n",
        "\n",
        "**Transpose:** We can write the transpose of matrix $A$:\n",
        "\n",
        "```\n",
        "np.einsum('ij -> ji', A) \n",
        "```\n",
        "\n",
        "**Trace:** We can write the trace of matrix $A$:\n",
        "\n",
        "```\n",
        "np.einsum('ii -> ', A) \n",
        "```\n",
        "\n",
        "**Diagonal:** We can write the diagonal of matrix $A$:\n",
        "\n",
        "```\n",
        "np.einsum('ii -> i', A) \n",
        "```\n",
        " \n",
        "**Matrix product:** We can write the multiplication of matrices $A$ and $B$ as:\n",
        "\n",
        "```\n",
        "np.einsum('ij, jk -> ik', A, B)\n",
        "```\n",
        "\n",
        "Note that $j$ in both matrices $A$ and $B$ should be the same size. \n",
        "\n",
        "**Batched matrix product (or why bothering):** All of the functions we performed above are built in numpy (np.tranpose, np.trace, np.matmul), however, when you want to do more complex operations, it might become less readable and computationaly efficient. Let's introduce a three dimensional matrix $H$ with indices $b,j,k$, where the first dimension is the batch (training example) dimension. In einsum, we can then write:\n",
        "\n",
        "```\n",
        "np.einsum('ij, bjk -> bik', A, H)\n",
        "```\n",
        "\n",
        "In order to perform a batched matrix multiplication where we multiple over the second dimension in the first marix and second dimension in the second matrix. The result is a new three dimensional matrix where the first dimension is the first dimension from $H$ and second is the first dimension from $A$ and last dimension the last dimension from $H$. This is a very simple one line (and readable) way to do matrix operations that will be very useful for neural network code. \n",
        "\n",
        "\n",
        "#### _**Tips and tricks when using einsum**_\n",
        "\n",
        "At the beginning, einsum might be a bit difficult to work with. The most important thing to do when using it is keeping track of the dimensions of your input and output matrices. An easy way to keep track of these dimensions is by using some sort of naming convention. Just like in the batched matrix product above we used $b$ to denote the batch dimension. In all the functions of this notebook, we leave some convention of names of indexes for the einsum in the explanation of the functions. We hope you find them useful!\n",
        "\n",
        "There are some other useful resources to understand numpy.einsum:\n",
        "\n",
        "* [Olexa Bilaniuk's great blogpost on einsum]( https://obilaniu6266h16.wordpress.com/2016/02/04/einstein-summation-in-numpy/ )\n",
        "* [Stackoverflow answer to: Understanding NumPy's einsum]( https://stackoverflow.com/q/26089893/8899404 )\n",
        "* [Jessica Stringham post on einsum]( https://jessicastringham.net/2018/01/01/einsum/ )\n",
        "* [Slides of einstein summation from oxford]( http://www-astro.physics.ox.ac.uk/~sr/lectures/vectors/lecture10final.pdfc )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bTMdY5DzFtHY"
      },
      "source": [
        "# Forward pass\n",
        "\n",
        "The forward pass has been implemented for you. Please note how we have used einsum to perform the affine tranformation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XTfP_fZEFtHY"
      },
      "source": [
        "#### Indices convention.\n",
        "\n",
        "* $i$: input - layer $l$ dimension.\n",
        "* $o$: output - layer $l+1$ dimension.\n",
        "* $b$: batch size dimension.\n",
        "\n",
        "<u>Attention</u>! \n",
        "\n",
        "By convention we consider column vectors.\n",
        "Depending on your implementation,\n",
        "sometimes you will need to transpose the matrix/vector dimensions. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cnlRKk-RFtHa"
      },
      "source": [
        "#### Matrices, Sums and Indices\n",
        "\n",
        "<u>Remember</u>!\n",
        "\n",
        "When we compute a matrix-vector product, the inner indices need to match; if we have $W \\in R^{K \\times J}$ and $z \\in R^{I}$, we can compute the matrix-vector product only if\n",
        "   *  $J=I$, then $Wz$ or \n",
        "   *  $K=I$, then $W^Tz$.\n",
        "   \n",
        "We need to transpose the matrix in the second case. Why?. Hint: inner indices matching).\n",
        "In general these two matrix-vector products are different. So pay attention to dimensions!\n",
        "\n",
        "Similarly, when we sum a matrix and a vector over a dimension, we can sum only if the dimensions match. Given the summation:\n",
        "    $$\\sum_{i=1}^I w_{ki}~z_{i},$$\n",
        "we can sum only if $J=I$.\n",
        "\n",
        "Index Contraction: after summing over an index, the index is contracted and the output is no more a function of that index. This means that if we sum over $i$, the result will be an object without index $i$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hAqTEP5GFtHb"
      },
      "source": [
        "#### Unpacking the forward pass. \n",
        "\n",
        "For each layer we want to compute a transformation between the activated units $z$ and the layer parameters $(W, b)$. In particular such transformation is an affine one, of the form:\n",
        "$$a^{(l)} = W^{(l)} z^{(l-1)} + b^{(l)},$$\n",
        "followed by a non-linear function (activation)\n",
        "$$z^{(l)} = h(a^{(l)}).$$\n",
        "\n",
        "If $W_{oi}$ is an element in a matrix with $O$ rows and $I$ columns, \n",
        "$z_{i}$ is a value in a vector of $I$ units and \n",
        "$b_o$ is a value in a vector of $O$ biases, \n",
        "we can write the affine transformation in different ways:\n",
        "\n",
        "* *Explicit notation*: $$\\sum_{i=1}^{I} w_{oi}~z_{i} + b_o,   \\quad \\forall o=1,...O$$\n",
        "* *Matrix notation*: $$ Wz + b$$ or $$z^{T}W + b$$\n",
        "\n",
        "\n",
        "Given that we have only two indices ($i$ and $o$), after contracting $i$, the output will have dimension $O$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MSelaywwFtHd"
      },
      "source": [
        "#### Not so fast!\n",
        "\n",
        "In Deep Learning we have another dimension: the batch size for the data $x$. \n",
        "This additional dimension is needed to pack together samples from the dataset and parallelize computations.\n",
        "\n",
        "So we typically work with matrices $W_{io}$ (notice how the indices are switched now) and vectors $z_i$ in batches, that we write $X = \\{x_{bi}\\}^{B, I}_{i=0, b=0}$ or $Z = \\{z_{bi}\\}^{B, I}_{i=0, b=0}$.\n",
        "\n",
        "Why do we use $W_{io}$ and not $W_{oi}$ as before? In principle we can use both. In practice, given that we want the batch size as first dimension, it is simpler to use $W_{io}$ to match the inner dimensions.\n",
        "As we said before, depending on your specific implementation, you will need to transpose some matrices.\n",
        "\n",
        "$x_{bi}$ means that we process in parallel a batch of $B$ samples (for example $B$=64 images, where each image is a sample) with dimensionality $I$ (for images this dimension is between $10^{3}$ - $10^{6}$). The batch size cannot be too large (Why?) and shouldn't be too small (Why?)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QKgTZL-WFtHe"
      },
      "source": [
        "#### einsum\n",
        "\n",
        "If you try to write in numpy a matrix-matrix product or a matrix-tensor product, you will notice that things get coumbersome fast. Additionally, your data could be a structured object with 3/4/5 dimensions ($x_{bijk}$). It's easy to get confused and make some mistakes, in particular when some of such dimensions have the same numerical value but different meaning (imagine a batch of 100 samples with dimension 100).\n",
        "\n",
        "The **einsum** function is explicitly summing over the dimension of choice taking care of the details related to batching in an efficient way. There is a striking similarity between the einsum notation and the explicit notation we have seen before. \n",
        "\n",
        "In the simple example we are working on, using einsum, matmul, or summing over an index is basically the same. \n",
        "For more complex problems in computer vision, natural language processing, generative modeling, etc. einsum helps dealing with the details."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1BRU4bAVFtHe"
      },
      "source": [
        "#### Putting it all together.\n",
        "\n",
        "Ok! Here we are. Let's write again the explicit form for the batch case. As before, $W \\in R^{I \\times O}$ (or $W^{T} \\in R^{O \\times I}$) is a matrix, $z_{bi}$ is a matrix (or a collection of $b$ row vectors), and $b_o$ is a vector (in practice the bias vector will be broadcasted for all the samples in a batch).\n",
        "\n",
        "* *Explicit notation*: \n",
        "$$\\sum_{i=1}^I z_{bi}~w_{io} + b_{o}, \\quad \\forall o, \\quad \\forall b$$\n",
        "Before, contracting $i$, we ended up with an object of dimension $o$. Now we end up with an object of dimensions $b~\\times~o$.\n",
        "\n",
        "\n",
        "* *Matrix notation*:\n",
        "$$ Z W + b $$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wQC5NSwQFtHe"
      },
      "source": [
        "def forward_pass(x, NN, activations):\n",
        "    \"\"\"\n",
        "    This function performs a forward pass. \n",
        "    It saves lists for both affine transforms of units (a) and activated units (z)\n",
        "    Input:\n",
        "    x: The input of the network             (np.array of shape: (batch_size, number_of_features))\n",
        "    NN: The initialized neural network      (tuple of list of matrices)\n",
        "    activations: the activations to be used (list of functions, same len as NN)\n",
        "\n",
        "    Output:\n",
        "    a: A list of affine transformations, that is, all x*w+b.\n",
        "    z: A list of activated units (ALL activated units including input x and output y).\n",
        "    \n",
        "    Shapes for the einsum:\n",
        "    b: batch size\n",
        "    i: size of the input hidden layer (layer l)\n",
        "    o: size of the output (layer l+1)\n",
        "    \"\"\"\n",
        "    z = [x]\n",
        "    a = []\n",
        "    \n",
        "    for l in range(len(NN[0])):\n",
        "        \n",
        "        # layer l parameters (W, bias)\n",
        "        W = NN[0][l]\n",
        "        bias = NN[1][l]\n",
        "        \n",
        "        # \\sum_{i} z^{l}_{bi} W^{l}_{io} in explicit notation\n",
        "        # z * W                          in matrix notation\n",
        "        Wz = np.einsum('bi, io -> bo', z[l], W)\n",
        "        \n",
        "        # z * W + bias\n",
        "        Wzb = Wz + bias\n",
        "        \n",
        "        a.append(Wzb)                  # The affine transform z*w+bias\n",
        "        z.append(activations[l](a[l])) # The non-linearity    \n",
        "    \n",
        "    return a, z"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-XyXBD37FtHk"
      },
      "source": [
        "# Forward pass unit test\n",
        "\n",
        "Below is a piece of code that takes a very particular setting of the network and inputs and test whether it gives the expected results."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k0miqRUAFtHl"
      },
      "source": [
        "ACT_F_UT = [Linear, Linear]\n",
        "test_a, test_z = forward_pass(np.array([[1,1,1]]), NN_UT, ACT_F_UT) # input has shape (1, 3) 1 batch, 3 features\n",
        "\n",
        "# Checking shapes consistency\n",
        "assert np.all(test_z[0]==np.array([1,1,1])) # Are the input vector and the first units the same?\n",
        "assert np.all(test_z[1]==test_a[0])         # Are the first affine transformations and hidden units the same?\n",
        "assert np.all(test_z[2]==test_a[1])         # Are the output units and the affine transformations the same?\n",
        "\n",
        "# Checking correctnes of values\n",
        "# First layer, calculate np.sum(np.array([1,1,1])*np.array([1,1,1]))+1 = 4\n",
        "assert np.all(test_z[1] == 4.)\n",
        "# Second layer, calculate np.sum(np.array([4,4,4,4,4])*np.array([1,1,1,1,1]))+1 = 21\n",
        "assert np.all(test_z[2] == 21.)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "faCxhfFnFtHp"
      },
      "source": [
        "# Loss functions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6JAc_ApuFtHq"
      },
      "source": [
        "In order to perform a backward pass we need to define a loss function and its derivative with respect to the output of the neural network $y$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I2eDYKvAFtHq"
      },
      "source": [
        "def squared_error(t, y, derivative=False):\n",
        "    \"\"\"\n",
        "    Computes the squared error function and its derivative \n",
        "    Input:\n",
        "    t:      target (expected output)          (np.array)\n",
        "    y:      output from forward pass (np.array, must be the same shape as t)\n",
        "    derivative: whether to return the derivative with respect to y or return the loss (boolean)\n",
        "    \"\"\"\n",
        "    if np.shape(t)!=np.shape(y):\n",
        "        print(\"t and y have different shapes\")\n",
        "    if derivative: # Return the derivative of the function\n",
        "        return (y-t)\n",
        "    else:\n",
        "        return 0.5*(y-t)**2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SrwSJ2UWFtHu"
      },
      "source": [
        "## Exercise d) Implement cross entropy loss\n",
        "\n",
        "Insert code below to implement cross-entropy loss for general dimensionality of $t$."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6nMuxyfzFtHv"
      },
      "source": [
        "def cross_entropy_loss(t, y, derivative=False):\n",
        "    \"\"\"\n",
        "    Computes the cross entropy loss function and its derivative \n",
        "    Input:\n",
        "    t:      target (expected output)          (np.array)\n",
        "    y:      output from forward pass (np.array, must be the same shape as t)\n",
        "    derivative: whether to return the derivative with respect to y or return the loss (boolean)\n",
        "    \"\"\"\n",
        "    ## Insert code here"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}