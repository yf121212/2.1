{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "anaconda-cloud": {},
    "colab": {
      "name": "2.1-FNN-NumPy.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8fAF5ew4FtHy"
      },
      "source": [
        "# Backward pass "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VG0W3mxGFtHz"
      },
      "source": [
        "## Exercise e) Complete code for backward pass\n",
        "\n",
        "Below is a implementation of the backward pass with some lines removed. Insert the missing lines."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0oUWm98uFtH0"
      },
      "source": [
        "#### Useful resources.\n",
        "\n",
        "Math intro:\n",
        "* https://www.deeplearningbook.org/contents/mlp.html\n",
        "* http://neuralnetworksanddeeplearning.com/chap2.html\n",
        "* http://pandamatak.com/people/anand/771/html/node37.html\n",
        "\n",
        "Code intro:\n",
        "* https://cs231n.github.io/neural-networks-case-study/#grad"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J-Tp1sP4FtH1"
      },
      "source": [
        "<u>Attention!</u>\n",
        "\n",
        "This is a difficult topic. It's normal to be confused and not immediately grasp the complete algorithm. Just work your way through it writing down the equations, writing down a small network explicitly, and thinking about the model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WWme3YQcFtH2"
      },
      "source": [
        "#### Unpacking the backward pass.\n",
        "\n",
        "Before we learned how to **forward** data input $X$ in a parametric model $W$ to obtain a final output $y$ and build a loss function $E(t, y)$ that measures how far our prediction $y$ is from the target $t$.\n",
        "For a generic FFN at layer $l$ we can write:\n",
        "\n",
        "$$a^{(l)} = W^{(l)} z^{(l-1)} + b^{(l)},$$\n",
        "$$z^{(l)} = h(a^{(l)}),$$\n",
        "where $z^{(0)}=x$ and $z^{(L)} = y$.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O5YDMa9lFtH2"
      },
      "source": [
        "#### Backpropagation.\n",
        "\n",
        "As the name suggests, the backpropagation algorithm is a procedure to adjust the model parameters $(W, b)$ \n",
        "<u>*propagating backward a measure of error*</u> such that our prediction $y$ is as close as possible to the target $t$.\n",
        "\n",
        "The proxy we use to measure closeness between $y$ and $t$ is a loss function $E(t, y)$. \n",
        "\n",
        "<u>Attention!</u>\n",
        "This loss is a function of the model parameters through $y = g(x, w, b)$. \n",
        "When we optimize wrt. the model parameters $(W, b)$ we can write the loss as a function of such parameters $E(w, b)$ because $y=g(w, b)$ and $t$ and $x$ are fixed input and output.\n",
        "\n",
        "In practice, backpropagation is a gradient based optimization strategy. \n",
        "Our goal is to compute all the partial derivatives $\\dfrac{\\partial E(w, b)}{\\partial w^l_{ji}}$ and $\\dfrac{\\partial E(w, b)}{\\partial b^l_{j}}$ for each layer $l$ and each unit $i$ and $j$ in the network.\n",
        "\n",
        "Computing the partial derivative wrt. the parameters directly is not easy. \n",
        "The loss depends in a non-linear and hierarchical way from the parameters. What backpropagation give us is an efficient and systematic way to compute the partial derivative as a function of intermediate quantities. The most important of such quantities is the error $\\delta_l$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f-a_KrnvFtH3"
      },
      "source": [
        "#### Error propagation - final layer $L$.\n",
        "\n",
        "Consider the final layer $L$.\n",
        "The partial derivatives can be written as:\n",
        "\n",
        "$$\\dfrac{\\partial E(w, b)}{\\partial w^{L}_{ji}} =\n",
        "\\dfrac{\\partial E}{\\partial z^{L}_j}\n",
        "\\dfrac{\\partial z^{L}_j}{\\partial w^{L}_{ji}} = \n",
        "\\left(\n",
        "\\color{blue}{\\dfrac{\\partial E}{\\partial z^{L}_j}\n",
        "\\dfrac{\\partial z^{L}_j}{\\partial a^{L}_j}}\n",
        "\\right)\n",
        "\\dfrac{\\partial a^{L}_j}{\\partial w^{L}_{ji}} = \n",
        "\\color{blue}{\\delta^{L}_j}~\\dfrac{\\partial a^{L}_j}{\\partial w^{L}_{ji}} = \n",
        "\\delta^{L}_j~z^{L-1}_{i},\n",
        "$$\n",
        "\n",
        "where\n",
        "$a^{L}_j = \\sum_i w^{L}_{ji}~z^{L-1}_i + b^{L}_j$ and $z^{L}_j = h(a^{L}_j).$ \n",
        "\n",
        "$\\delta^{L}_j = \\partial E / \\partial a^{L}_j$ is the error at layer $L$ for unit $j$ and it is what we want to backpropagate. \n",
        "You see that it can be easily computed: \n",
        "$ \\partial E / \\partial z^{L}_j$\n",
        "is just the partial derivative of the loss wrt. to the activation.\n",
        "$ \\partial z^{L}_j / \\partial a^{L}_j$\n",
        "is the partial derivative of the activation wrt. the output of the affine transformation.\n",
        "You already know how to compute these derivative (How?) and consequently you know $\\delta^{L}_j$.\n",
        "\n",
        "For the biases in the final layer the computation is basically the same:\n",
        "\n",
        "$$\\dfrac{\\partial E(w, b)}{\\partial b_{j}} =\n",
        "\\left(\\dfrac{\\partial E}{\\partial z_j}\n",
        "\\dfrac{\\partial z_j}{\\partial a_j}\\right)\n",
        "\\dfrac{\\partial a_j}{\\partial b_{j}} = \n",
        "\\delta_j~\\dfrac{\\partial a_j}{\\partial b_{j}} = \n",
        "\\delta_j.\n",
        "$$\n",
        "\n",
        "You can see that $\\delta_j$ is the same we computed, and the only thing that changes is the partial derivative wrt $w_{ji}$ and $b_j$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BOvIa4zKFtH4"
      },
      "source": [
        "#### Error propagation - layer $l$.\n",
        "\n",
        "For a generic layer $l$ we want to derive a recursive formula for the error $\\delta^{l}_j$.\n",
        "In particular, if we at layer $l$ and unit $j$, we consider the $k$ downstream units influenced by $j$:\n",
        "\n",
        "$$\\dfrac{\\partial E(w, b)}{\\partial w^l_{ji}} =\n",
        "\\sum_k\n",
        "\\dfrac{\\partial E}{\\partial a^{l+1}_k}\n",
        "\\dfrac{\\partial a^{l+1}_k}{\\partial w^l_{ji}} =\n",
        "\\left(\n",
        "\\color{green}{\\sum_k\n",
        "\\dfrac{\\partial E}{\\partial a^{l+1}_k}\n",
        "\\dfrac{\\partial a^{l+1}_k}{\\partial z^l_j}\n",
        "\\dfrac{\\partial z^l_j}{\\partial a^l_j}}\n",
        "\\right)\n",
        "\\dfrac{\\partial a^l_j}{\\partial w^l_{ji}} = \n",
        "\\color{green}{\\delta^{l}_j}~\\dfrac{\\partial a^{l}_j}{\\partial w^{l}_{ji}} = \n",
        "\\delta^l_j~z^{l-1}_{i},\n",
        "$$\n",
        "\n",
        "where $a^{l}_j = \\sum_i w^{l}_{ji}~z^{l-1}_i + b^{l}_j$ and $z^{l}_j = h(a^{l}_j).$\n",
        "\n",
        "Notice how the terms in $\\delta^{l}_j$ are easy to compute: $\\partial E  / \\partial a^{l+1}_k = \\delta^{l+1}_k$ is known (we start from the last layer). $\\partial a^{l+1}_k / \\partial z^{l}_j$ is something new, but we can easily compute this term too. \n",
        "We notice that:\n",
        "$$a^{l+1}_k = \\sum_j w^{l+1}_{kj} z^{l}_{j} + b_k,$$\n",
        "and $\\partial a^{l+1}_k / \\partial z^{l}_j = w^{l+1}_{kj}$. \n",
        "Finally $\\partial z^l_j / \\partial a^l_j$ is just the partial derivative of the activation (as before).\n",
        "\n",
        "We can now write the general recursive form for error propagation:\n",
        "\n",
        "$$\\delta^{l}_j = \\left(\\sum_k \\delta^{l+1}_k w^{l+1}_{kj}\\right) \\dfrac{\\partial z^l_j}{\\partial a^l_j}$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uDjCFCCDFtH5"
      },
      "source": [
        "#### Putting it all together.\n",
        "\n",
        "Here we are! Everything boils down to recursively finding the errors $\\delta^{l}$ starting from the last layer $L$ backward. When you have $\\delta^{l}$, you just need a final multiplication with activations to find the partial derivative wrt the parameters for the layer.\n",
        "Let's write the steps in matrix form:\n",
        "\n",
        "* Errror for the last layer $L$:\n",
        "$$\\delta^{L} = \\nabla_z E \\circ h'(a^{L}).$$\n",
        "* Error for any layer $l$:\n",
        "$$\\delta^l = ((W^{l+1})^{T} \\delta^{l+1}) \\circ h'(a^{l}).$$\n",
        "* Partial derivative cost for $w$: \n",
        "$$\\dfrac{\\partial E}{\\partial w^{l}} = \\delta^{l} z^{l-1}.$$\n",
        "* Partial derivative cost for $b$:\n",
        "$$\\dfrac{\\partial E}{\\partial b^{l}} = \\delta^{l}.$$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "E5VYbaVqFtH5"
      },
      "source": [
        "def backward_pass(x, t, y, z, a, NN, activations, loss_f):\n",
        "    \"\"\"\n",
        "    This function performs a backward pass ITERATIVELY. It saves lists all of the derivatives in the process\n",
        "    \n",
        "    Input:\n",
        "    x:           The input used for the batch                (np.array)\n",
        "    t:           The observed targets                        (np.array, the first dimension must be the same to x)\n",
        "    y:           The output of the forward_pass of NN for x  (np.array, must have the same shape as t)\n",
        "    \n",
        "    a:           The affine transforms from the forward_pass (np.array) # a^{l+1}= W^{l} z^{l} + b^{l}\n",
        "    z:           The activated units from the forward_pass (np.array)   # z^{l+1}=f(a^{l+1})\n",
        "    \n",
        "    activations: The activations to be used                  (list of functions)\n",
        "    loss_f:        The loss function to be used                (one function)\n",
        "    \n",
        "    Output:\n",
        "    g_w: A list of gradients for every weight\n",
        "    g_b: A list of gradients for every bias\n",
        "    \n",
        "    Shapes for the einsum:\n",
        "    b: batch size\n",
        "    i: size of the input hidden layer (layer l)\n",
        "    o: size of the output (layer l+1)\n",
        "    \"\"\"\n",
        "    \n",
        "    BS = x.shape[0] # Implied batch shape \n",
        "    \n",
        "    # Process the last layer - Reference: Error propagation - final layer $L$.\n",
        "    \n",
        "    # First, let's compute the list of derivatives of z with respect to a\n",
        "    # these derivative are standardized and automatically handled by the activations functions defined above.\n",
        "    d_a = []\n",
        "    # dz/da\n",
        "    for i in range(len(activations)):\n",
        "        d_za = activations[i](a[i], derivative=True)\n",
        "        d_a.append(d_za)\n",
        "    \n",
        "    # Second, let's compute the derivative of the loss function with respect to z\n",
        "    # targets\n",
        "    t = t.reshape(BS, -1)\n",
        "    \n",
        "    # derivative loss wrt y\n",
        "    # dE/dy  where y=z[-1]\n",
        "    d_loss = 0      # <- Insert correct expression here\n",
        "    \n",
        "     \n",
        "    # Third, let's compute the derivative of the biases and the weights\n",
        "    g_w   = [] # List to save the gradient w.r.t. the weights\n",
        "    g_b   = [] # List to save the gradients w.r.t. the biases\n",
        "    \n",
        "    # delta : measure of error in the final layer L\n",
        "    # delta = dE/dy * dy/da\n",
        "    delta = np.einsum('bo, bo -> bo', d_loss, d_a[-1]) # loss shape: (b, o); pre-activation units shape: (b, o) hadamard product\n",
        "    \n",
        "    # affine transformation\n",
        "    # a = W z + b\n",
        "    \n",
        "    # dE/dw = (dE/dy * dy/da) da/dw = delta * da/dw\n",
        "    # notice how the gradients wrt the weights have dimension (batch, input_dim, output_dim)\n",
        "    g_w.append(np.mean(np.einsum('bo, bi -> bio', delta, z[-2]), axis=0)) # delta shape: (b, o), activations shape: (b, h)\n",
        "    \n",
        "    # dE/db = (dE/dy * dy/da) da/db = delta * da/db\n",
        "    g_b.append(np.mean(delta, axis=0))\n",
        "    \n",
        "    \n",
        "    # Process all the other layers - Reference: Error propagation - layer $l$\n",
        "    for l in range(1, len(NN[0])):\n",
        "        \n",
        "        W = NN[0][-l] \n",
        "        # dE/dz^{l} = dE/da^{l+1} * da^{l+1}/dz^{l} = delta^{l+1} * w^{l+1}\n",
        "        d_E_d_z = np.einsum('bo, io -> bi', delta, W)          # Derivative of the loss with respect to an activated layer d_E_d_z. \n",
        "                                                               #  delta shape: as above; weights shape: (i, o)\n",
        "                                                               # Delta: d_E_d_z (element-wise mult) derivative of the activation layers\n",
        "                                                               #  delta shape: as above; d_z shape: (b, i)  \n",
        "        \n",
        "        # delta : measure of error for a generic layer l \n",
        "        # dE/dz * dz/da \n",
        "        delta = 0      # <- Insert correct expression here \n",
        "        \n",
        "        # affine transformation\n",
        "        # a = Wz + b\n",
        "        \n",
        "        # dE/dw = delta * da/dw\n",
        "        g_w.append(np.mean(np.einsum('bo, bi -> bio', delta, z[-l-2]), axis=0)) # Derivative of cost with respect to weights in layer l:\n",
        "        \n",
        "        # dE/db = delta                                                                 # delta shape: as above; activations of l-1 shape: (b, i)\n",
        "        g_b.append(np.mean(delta, axis=0))\n",
        "        \n",
        "    return g_b[::-1], g_w[::-1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D7d7qK0uFtH9"
      },
      "source": [
        "# Backward pass unit test"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SGiqQ0ePFtH9"
      },
      "source": [
        "We are going to perform the unit test of the backward pass with a finite difference estimation, make sure to read the description of the function and that you understand it well:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uiJDAZT6FtH_"
      },
      "source": [
        "## Exercise f) Test correctness of derivatives with finite difference method\n",
        "\n",
        "Write a small function that uses [the finite difference method](https://en.wikipedia.org/wiki/Finite_difference_method) to test whether the backpropation implementation is working. In short we will use\n",
        "$$\n",
        "\\frac{\\partial E(w)}{\\partial w_{ij}^{(l)}} \\approx \\frac{E(v)-E(w)}{dw}\n",
        "$$\n",
        "for $dw \\ll 1$ and $v$ is the same network as $w$ apart from $v_{ij}^{(l)} = w_{ij}^{(l)} + dw$.\n",
        "\n",
        "As arguments the function should take: some data $x$ and $t$ as in the example above, the network including activations, the indices $i$, $j$, $l$ of the weight we investigate and $dw$ and return the right hand side of the expression above.\n",
        "\n",
        "_Insert your code in the cell below._\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "Om8UfjeZFtH_"
      },
      "source": [
        "# Insert your finite difference code here\n",
        "def finite_difference(x, t, NN, activations, indexes, dw=1e-10):\n",
        "    \"\"\"\n",
        "    This function compute the finite difference between\n",
        "    \n",
        "    Input:\n",
        "    x:           The input used for the batch                (np.array)\n",
        "    t:           The observed targets                        (np.array, the first dimension must be the same to x)\n",
        "    \n",
        "    NN: The initialized neural network                       (tuple of list of matrices)\n",
        "    activations: The activations to be used                  (list of functions)\n",
        "    \n",
        "    indexes: the indexes of the parameter we want to perturb (tuple of integers)\n",
        "             v^{l}_{ji} = w^{l}_{ji} + dw\n",
        "    \n",
        "    dw: the size of the difference                           (float)\n",
        "    \n",
        "    Output:\n",
        "    finite_difference: the magnitude of the difference       (float) \n",
        "    \"\"\"\n",
        "    \n",
        "    from copy import deepcopy\n",
        "    # l layer\n",
        "    # i input dim\n",
        "    # j output dim\n",
        "    \n",
        "    (l, i, j) = indexes\n",
        "    \n",
        "    _, zw = forward_pass(x, NN, activations)\n",
        "    Ew=squared_error(t, zw[-1])\n",
        "    \n",
        "    NNv = deepcopy(NN)\n",
        "    NNv[0][l][i, j] = 0 # <- Insert correct expression \n",
        "    \n",
        "    _, zv = 0           # <- Insert correct expression\n",
        "    Ev= 0               # <- Insert correct expression\n",
        "    \n",
        "    finite_difference = (Ev - Ew) / dw\n",
        "    \n",
        "    return finite_difference"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WcLlsejQFtID"
      },
      "source": [
        "Once you have implemented the function you can compare this number with the left hand side computed by the implementation above.\n",
        "\n",
        "Try for different parameters and different values of $dw$. Scan over a range of $dw$ values. Why does the method break down for really small $dw$?\n",
        "\n",
        "_Insert your written answer here._\n",
        "\n",
        "Finite differences gives us gradients without computing gradients explicitly. Why don't we use it in practice then?\n",
        "\n",
        "_Insert your written answer here._"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PJ59iNHmFtID"
      },
      "source": [
        "Below is reference code that computes the finite differences for all parameters."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "5Zie8Q8xFtIE"
      },
      "source": [
        "def finite_diff_grad(x, NN, ACT_F, epsilon=None):\n",
        "    \"\"\"\n",
        "    Finite differences gradient estimator: https://en.wikipedia.org/wiki/Finite_difference_method\n",
        "    The idea is that we can approximate the derivative of any function (f) with respect to any argument (w) by evaluating the function at (w+e)\n",
        "    where (e) is a small number and then computing the following opertion (f(w+e)-f(w))/e . Note that we would need N+1 evaluations of\n",
        "    the function in order to compute the whole Jacobian (first derivatives matrix) where N is the number of arguments. The \"+1\" comes from the\n",
        "    fact that we also need to evaluate the function at the current values of the argument.\n",
        "    \n",
        "    Input:\n",
        "    x:       The point at which we want to evaluate the gradient\n",
        "    NN:      The tuple that contains the neural network\n",
        "    ACT_F:   The activation functions in order to perform the forward pass\n",
        "    epsilon: The size of the difference\n",
        "    \n",
        "    Output:\n",
        "    Two lists, the first one contains the gradients with respect to the weights, the second with respect to the biases\n",
        "    \"\"\"\n",
        "    from copy import deepcopy\n",
        "    \n",
        "    if epsilon == None:\n",
        "        epsilon = np.finfo(np.float32).eps # Machine epsilon for float 32\n",
        "        \n",
        "    grads = deepcopy(NN)               # Copy of structure of the weights and biases to save the gradients                        \n",
        "    _ , test_z = forward_pass(x, NN_UT, ACT_F_UT) # We evaluate f(x)\n",
        "    \n",
        "    for e in range(len(NN)):                       # Iterator over elements of the NN:       weights or biases\n",
        "        for h in range(len(NN[e])):                # Iterator over the layer of the element: layer number\n",
        "            for r in range(NN[e][h].shape[0]):     # Iterator over                           row number\n",
        "                for c in range(NN[e][h].shape[1]): # Iterator over                           column number \n",
        "                    NN_copy             = deepcopy(NN)    \n",
        "                    NN_copy[e][h][r,c] += epsilon\n",
        "                    _, test_z_eps       = forward_pass(x, NN_copy, ACT_F)     # We evaluate f(x+eps)\n",
        "                    grads[e][h][r,c]    = (test_z_eps[-1]-test_z[-1])/epsilon # Definition of finite differences gradient\n",
        "    \n",
        "    return grads[0], grads[1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "GxkISRCmFtIH",
        "scrolled": true
      },
      "source": [
        "### Unit test \n",
        "\n",
        "## First lest's compute the backward pass using our own function\n",
        "# Forward pass\n",
        "test_a, test_z = forward_pass(np.array([[1,1,1]]), NN_UT, ACT_F_UT)\n",
        "# Backward pass\n",
        "test_g_b, test_g_w = backward_pass(np.array([[1,1,1]]), np.array([20]), test_a[-1], test_z, test_a, NN_UT, ACT_F_UT, squared_error)\n",
        "# Estimation by finite differences\n",
        "test_fdg_w, test_fdg_b = finite_diff_grad(np.array([[1,1,1]]), NN_UT, ACT_F_UT)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "Ylz9fQHsFtIL"
      },
      "source": [
        "# Test whether the weights and biases are all equal as the ones we estimated using back propagation\n",
        "for l in range(len(test_g_w)):\n",
        "    assert np.allclose(test_fdg_w[l], test_g_w[l])\n",
        "    assert np.allclose(test_fdg_b[l], test_g_b[l])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WgBi8GOSFtIN"
      },
      "source": [
        "# Training and validation\n",
        "\n",
        "We are ready to train some neural networks! Below we give some example initializations and a training loop. Try it out. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "woWYpdw6FtIO"
      },
      "source": [
        "# Initialize an arbitrary neural network\n",
        "#L  = [3, 16, 1]\n",
        "L  = [1, 8, 1]\n",
        "NN = init_NN(L)\n",
        "#NN = init_NN_glorot(L, uniform=True)\n",
        "#NN = init_NN_he_ReLU(L, uniform=True)\n",
        "\n",
        "ACT_F = [ReLU, Linear]\n",
        "#ACT_F = [Tanh, Linear]\n",
        "\n",
        "# Recommended hyper-parameters for 1-D: \n",
        "# L  = [1, 8, 1]\n",
        "# EPOCHS = 10000\n",
        "# BATCH_SIZE = 128 \n",
        "# LEARN_R = 2.5e-1 for Tanh and LEARN_R = 1e-1 for ReLU\n",
        "\n",
        "# Recommended hyper-parameters for 3-D: \n",
        "# L  = [3, 16, 1] \n",
        "# EPOCHS = 10000\n",
        "# BATCH_SIZE = 128 \n",
        "# LEARN_R = 5e-2 for ReLU and LEARN_R = 1e-1 for Tanh\n",
        "\n",
        "### Notice that, when we switch from tanh to relu activation, we decrease the learning rate. This is due the stability of the gradients \n",
        "## of the activation functions."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mdqaqYBVFtIR"
      },
      "source": [
        "# Initialize training hyperparameters\n",
        "EPOCHS = 20000\n",
        "BATCH_SIZE = 128 \n",
        "LEARN_R = 1e-2 "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5kfg76GMFtIW",
        "scrolled": true
      },
      "source": [
        "train_loss = []\n",
        "val_loss = []\n",
        "\n",
        "for e in range(EPOCHS):\n",
        "    # Mini-batch indexes\n",
        "    idx = np.random.choice(x_train.shape[0], size=BATCH_SIZE)\n",
        "    # Forward pass\n",
        "    aff, units = forward_pass(x_train[idx,:], NN, ACT_F)\n",
        "    # Backward pass\n",
        "    g_b, g_w = backward_pass(x_train[idx,:], y_train[idx], units[-1], units, aff, NN, ACT_F, squared_error)\n",
        "    \n",
        "    # Stochastic gradient descent\n",
        "    for l in range(len(g_b)):\n",
        "        NN[0][l] -= LEARN_R*g_w[l]\n",
        "        NN[1][l] -= LEARN_R*g_b[l]\n",
        "        \n",
        "    # Training loss\n",
        "    _, units = forward_pass(x_train, NN, ACT_F)\n",
        "    # Estimate loss function\n",
        "    #print(np.max(squared_error(y_train, units[-1])))\n",
        "    train_loss.append(np.mean(squared_error(y_train, np.squeeze(units[-1]))))\n",
        "    \n",
        "    # Validation\n",
        "    # Forward pass\n",
        "    _, units = forward_pass(x_validation, NN, ACT_F)\n",
        "    # Estimate validation loss function\n",
        "    val_loss.append(np.mean(squared_error(y_validation, np.squeeze(units[-1]))))\n",
        "    \n",
        "    if e%500==0:\n",
        "        print(\"{:4d}\".format(e),\n",
        "              \"({:5.2f}%)\".format(e/EPOCHS*100), \n",
        "              \"Train loss: {:4.3f} \\t Validation loss: {:4.3f}\".format(train_loss[-1], val_loss[-1]))\n",
        "        \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VetyRWFwFtIY"
      },
      "source": [
        "plt.plot(range(len(train_loss)), train_loss);\n",
        "plt.plot(range(len(val_loss)), val_loss);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8OgmIrM9FtIb"
      },
      "source": [
        "# Testing\n",
        "\n",
        "We have kept the calculation of the test error separate in order to emphasize that you should not use the test set in optimization."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HmNi7S-vFtIc"
      },
      "source": [
        "_, units = forward_pass(x_test, NN, ACT_F)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7mmJOTSEFtIf"
      },
      "source": [
        "plt.scatter(y_test, units[-1]);\n",
        "plt.plot([np.min(y_test), np.max(y_test)], [np.min(y_test), np.max(y_test)], color='k');\n",
        "plt.xlabel(\"y\");\n",
        "plt.ylabel(\"$\\hat{y}$\");\n",
        "plt.title(\"Model prediction vs real in the test set, the close to the line the better\")\n",
        "plt.grid(True);\n",
        "plt.axis('equal');\n",
        "plt.tight_layout();\n",
        "\n",
        "print(\"Test loss:  {:4.3f}\".format(np.mean(squared_error(y_test, np.squeeze(units[-1])))))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ODi0WlmQFtIh"
      },
      "source": [
        "if D1:\n",
        "    plt.scatter(x_train[:,0], y_train, label=\"train data\");\n",
        "    plt.scatter(x_test[:,0], units[-1], label=\"test prediction\");\n",
        "    plt.scatter(x_test[:,0], y_test, label=\"test data\");\n",
        "    plt.legend();\n",
        "    plt.xlabel(\"x\");\n",
        "    plt.ylabel(\"y\");\n",
        "else:\n",
        "    plt.scatter(x_train[:,1], y_train, label=\"train data\");\n",
        "    plt.scatter(x_test[:,1], units[-1], label=\"test data prediction\");\n",
        "    plt.scatter(x_test[:,1], y_test, label=\"test data\");\n",
        "    plt.legend();\n",
        "    plt.xlabel(\"x\");\n",
        "    plt.ylabel(\"y\");"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zTBAmjsAFtIk"
      },
      "source": [
        "## Exercise g) Show overfitting, underfitting and just right fitting\n",
        "\n",
        "Vary the architecture and other things to show clear signs of overfitting (=training loss significantly lower than test loss) and underfitting (=not fitting enoung to training data so that test performance is also hurt).\n",
        "\n",
        "See also if you can get a good compromise which leads to a low validation loss. \n",
        "\n",
        "For this problem do you see any big difference between validation and test loss? The answer here will probably be no. Discuss cases where it is important to keep the two separate.\n",
        "\n",
        "_Insert written answer here._\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "tQZCn2dxFtIl"
      },
      "source": [
        "# Insert your code for getting overfitting, underfitting and just right fitting"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fYPZP-eTFtIo"
      },
      "source": [
        "# Next steps - classification\n",
        "\n",
        "It is straight forward to extend what we have done to classification. \n",
        "\n",
        "For numerical stability it is better to make softmax and cross-entropy as one function so we write the cross entropy loss as a function of the logits we talked about last week. \n",
        "\n",
        "Next week we will see how to perform classification in PyTorch."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qsVPul3QFtIo"
      },
      "source": [
        "## Exercise h) optional - Implement backpropagation for classification\n",
        "\n",
        "Should be possible with very few lines of code. :-)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "oC8QrI2tFtIp"
      },
      "source": [
        "# Just add code."
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}